<p align="center">
<img src="./assert/Framework.jpg" width="700" alt="MLLM-Tuning" />
</p>

# <p align="center">Awesome-MLLM-Tuning</p>

<p align="center"><em>Curated list of Multimodal Large Language Model (MLLM) Tuning resources, aligned with our work:</em><br><strong>Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model</strong></p>

<p align="center">
<a href="https://arxiv.org/abs/2503.04543"><img src="https://img.shields.io/badge/arXiv-2502.14881-b31b1b.svg" alt="arXiv Badge"></a>
<img src="https://badges.toozhao.com/badges/01JNMP5KB247X0F52D94216CT0/blue.svg" alt="Custom Badge" />
<a href="https://creativecommons.org/licenses/by-nc/4.0/"><img src="https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg" alt="License Badge"></a>
<a href="https://github.com/WenkeHuang/Awesome-MLLM-Tuning"><img src="https://img.shields.io/github/stars/WenkeHuang/Awesome-MLLM-Tuning?style=social" alt="GitHub stars"></a>
</p>

## üôå Abstract

Multi-modal Large Language Models (MLLMs) integrate visual and linguistic reasoning to address complex tasks such as image captioning and visual question answering. While MLLMs demonstrate remarkable versatility, MLLMs appears limited performance on special application. But tuning MLLMs for downstream tasks encounters two key challenges: Task-Expert Specialization, where distribution shifts between pre-training and target datasets constrain target performance, and Open-World Stabilization, where catastrophic forgetting erases the model general knowledge. In this work, we systematically review recent advancements in MLLM tuning methodologies, classifying them into three paradigms: (I) Selective Tuning, (II) Additive Tuning, and (III) Reparameterization Tuning. Furthermore, we benchmark these tuning strategies across popular MLLM architectures and diverse downstream tasks to establish standardized evaluation analysis and systematic tuning principles. Finally, we highlight several open challenges in this domain and propose future research directions.

## üìñ Paper

### Reparameterization Tuning

#### Structure Reparameterization Tuning

| Time    | Title                                                        | Venue | Paper | Code |
| ------- | ------------------------------------------------------------ | :---: | :---: | :--: |
| 2025.02 | **REMEDY: Recipe merging dynamics in large vision-language models** | ICLR‚Äô25 | [link](https://openreview.net/forum?id=iX7eHHE5Tx) | - |
|         | ****                                                         | ACL‚Äô24 | [link]() | [link]() |
|         | ****                                                         | EMNLP‚Äô24 | [link]() | [link]() |
|         | ****                                                         | ACL‚Äô24 | [link]() | [link]() |
|         | ****                                                         | ICML‚Äô24 | [link]() | [link]() |
|         | ****                                                         | arXiv‚Äô24 | [link]() | [link]() |
|         | ****                                                         | arXiv‚Äô24 | [link]() | [link]() |
|         | ****                                                         | COLM‚Äô24 | [link]() | [link]() |
|         | ****                                                         | ICLR‚Äô24 | [link]() | [link]() |
|         | ****                                                         | CVPR‚Äô24 | [link]() | [link]() |
|         | ****                                                         | arXiv‚Äô24 | [link]() | [link]() |
|         | ****                                                         | NeurIPS‚Äô24 | [link]() | [link]() |

#### Calibration Reparameterization Tuning

| Time    | Title                                                        | Venue | Paper | Code |
| ------- | ------------------------------------------------------------ | :---: | :---: | :--: |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |
|         | ****                                                         |       | [link]() | [link]() |

## üëã Contact

This repository is currently maintained by [Wenke Huang](https://wenkehuang.github.io/) üë®‚Äçüíª.  
If you have any questions, concerns, or suggestions regarding the contents of this repository or the resources shared here, feel free to reach out! I'm more than happy to assist you with any inquiries or help you navigate through the materials.  
Please don't hesitate to send an email to me at [wenkehuang@whu.edu.cn](mailto:wenkehuang@whu.edu.cn) üìß or [Wechat](assert/wechat.jpg) ü§ó.

## ü•≥ Citation

If you find this repository helpful for your research, we would greatly appreciate it if you could cite our papers. ‚ú®

```bibtex
@misc{MLLMTuning_arXiv25,
      title={Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model}, 
      author={Wenke Huang, Jian Liang, Xianda Guo, Yiyang Fang, Guancheng Wan, Xuankun Rong, Chi Wen, Zekun Shi,  Qingyun Li, Didi Zhu, Yanbiao Ma, Ke Liang, Bin Yang, He Li, Jiawei Shao, Mang Ye, Bo Du},
      year={2025},
      eprint={2503.04543},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
```








**You Only Live Once.**

**I hope that all players have fun.**
